---
title: "2.1_2.2"
author: "Juliet Cohen"
date: "10/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 2.1: dataRetrieval Package

This package helps retrieve USGS data directly from their website.

They use unique ID's for each stream gage, which are 8-15 digits depending on if they are surface-water sites or groundwater sites.

They use 5-digit parameter codes that specifies the measured parameter being requested, but not every station measures all parameters

- 00060 = discharge 
- 00065 = gage height
- 00010 = temperature
- 00045 = precip
- 00400 = pH

USGS stores daily values as statistical summaries of continuous data that is collected at time intervals such as every 15 minutes or hourly, and these stats are specified by a 5-digit code

- 00001 = max
- 00002 = min
- 00003 = mean
- 00008 = median

## Discharge time series for the Ventura River from 2019-10-01 to 2020-10-05 using an API

The following is a plot we created in class using this API.

```{r}
library(dataRetrieval)
library(tidyverse)

siteNumber <- "01491000"
parameterCd <- c("00010","00060")  # Temperature and discharge
statCd <- c("00001","00003")  # Mean and maximum
startDate <- "2012-01-01"
endDate <- "2012-05-01"

temperatureAndFlow <- readNWISdv(siteNumber, parameterCd, 
        startDate, endDate, statCd=statCd)

temperatureAndFlow <- renameNWISColumns(temperatureAndFlow)
names(temperatureAndFlow)

statInfo <- attr(temperatureAndFlow, "statisticInfo")
variableInfo <- attr(temperatureAndFlow, "variableInfo")
siteInfo <- attr(temperatureAndFlow, "siteInfo")

par(mar=c(5,5,5,5)) #sets the size of the plot window

plot(temperatureAndFlow$Date, temperatureAndFlow$Wtemp_Max,
  ylab=variableInfo$parameter_desc[1],xlab="" )
par(new=TRUE)
plot(temperatureAndFlow$Date, temperatureAndFlow$Flow,
  col="red",type="l",xaxt="n",yaxt="n",xlab="",ylab="",axes=FALSE
  )
axis(4,col="red",col.axis="red")
mtext(variableInfo$parameter_desc[2],side=4,line=3,col="red")
title(paste(siteInfo$station_nm,"2012"))
#legend("topleft", legend, variableInfo$param_units, 
#       col=c("black","red"),lty=c(NA,1),pch=c(1,NA))
```

Let's look at the water flow at the Ventura River from 2019 to 2020.

```{r}
siteNumber <- "11118500"
parameterCd <- "00060" 
startDate <- "2019-10-01"
endDate <- "2020-10-05"

discharge_ven <- readNWISdv(siteNumber, parameterCd, startDate, endDate)
#discharge

water_plot_ven <- ggplot(data = discharge_ven, aes(x = Date, y = X_00060_00003)) +
  geom_line() +
  theme(panel.grid = element_blank()) +
  scale_y_continuous(breaks = seq(0, 1400, by = 250)) +
  labs(title = "Ventura River Discharge 2019-2020",
       x = "Date",
       y = "Water Discharge (ft^3/s)")

water_plot_ven
```

![**Ventura River flows in March 2020**](VenturaAbove150March162020-5_12PM.jpg)

## 2.1: USGS Stream Data

### 1. Add the Santa Paula Creek gauge

```{r}
siteNumber <- "11113500"
parameterCd <- "00060" 
startDate <- "2019-10-01"
endDate <- "2020-10-05"

discharge_sp <- readNWISdv(siteNumber, parameterCd, startDate, endDate)
#discharge

water_plot_sp <- ggplot(data = discharge_sp, aes(x = Date, y = X_00060_00003)) +
  geom_line() +
  theme(panel.grid = element_blank()) +
  scale_y_continuous(breaks = seq(0, 350, by = 50)) +
  labs(title = "Santa Paula Creek Discharge 2019-2020",
       x = "Date",
       y = "Water Discharge (ft^3/s)")
  

water_plot_sp
```


### 2. The best way to query multiple sites using this R package is to use the readNWIds() function and set the argument siteNumber equal to a concatonated list of multiple sites. Then the dataframe that is output has data from both sites in the column site_no. The following code retreives data from the Ventura and Santa Paula sites. 

```{r}
# set the siteNumber equal to 2 sites so you can get both sites' data in one dataframe

siteNumber <- c("11113500", "11118500")
parameterCd <- "00060" 
startDate <- "2019-10-01"
endDate <- "2020-10-05"

discharge_ven_sp_2019_2020 <- readNWISdv(siteNumber, parameterCd, startDate, endDate)
#discharge_ven_sp_2019_2020

#colnames(discharge_ven_sp_2019_2020)

#check the class of the output
#class(discharge_ven_sp_2019_2020)

discharge_ven_sp_2019_2020_plot <- ggplot(discharge_ven_sp_2019_2020, aes(x = Date, y = X_00060_00003))+
  geom_line(aes(color = site_no)) +
  scale_color_discrete(name = "Site", labels = c("Santa Paula", "Ventura")) +
  theme(panel.grid = element_blank()) +
  labs(title = "Ventura & Santa Paula Discharge 2019-2020",
       x = "Date",
       y = "Water Discharge (ft^3/s)")

discharge_ven_sp_2019_2020_plot
```

### 3. Let's plot the stream flow response to the rain on 10/04, first as daily averages and then with data taken at 15-minute intervals.

Let's determine at what time the stream flow peaked (if any) at the two locations.

```{r}
siteNumber <- c("11113500", "11118500")
parameterCd <- "00060" 
startDate <- "2021-10-04"
endDate <- "2021-10-06"

# get data for daily summary data at each site during the storm

discharge_ven_sp_storm <- readNWISdv(siteNumber, parameterCd, startDate, endDate)

colnames(discharge_ven_sp_storm)

storm_discharge_plot_daily_summary <- ggplot(data = discharge_ven_sp_storm, aes(x = Date, y = X_00060_00003)) +
  geom_line(aes(color = site_no)) +
  theme(panel.grid = element_blank()) +
  labs(title = "Water Discharge from Ventura and Santa Paula gages daily summary values",
      x = "Date",
      y = "Water Discharge (ft^3/s") +
 scale_color_discrete(name = "Site", labels = c("Santa Paula", "Ventura"))

storm_discharge_plot_daily_summary
```

```{r}

# get data for 15-min intervals at each site during the storm

short_interval_data <- readNWISdata(sites = c("11113500", "11118500"), service = "iv", parameterCD = "00060", startDate = "2021-10-04T00:00Z", endDate = "2021-10-06T00:00Z")

#colnames(short_interval_data)

storm_discharge_plot_short_intervals <- ggplot(data = short_interval_data, aes(x = dateTime, y = X_00060_00000)) +
  geom_line(aes(color = site_no)) +
  theme(panel.grid = element_blank()) +
  labs(title = "Water Discharge from Ventura and Santa Paula gages at 15 min intervals",
      x = "Date & Time",
      y = "Water Discharge (ft^3/s)") +
 scale_color_discrete(name = "Site", labels = c("Santa Paula", "Ventura"))

storm_discharge_plot_short_intervals

# find the peak discharge, filter the dataset with discharge data from each site

# SP

SP_storm_discharge <- short_interval_data %>% 
  filter(site_no == 11113500)

max(SP_storm_discharge$X_00060_00000)

# Ven

Ven_storm_discharge <- short_interval_data %>% 
  filter(site_no == 11118500)

max(Ven_storm_discharge$X_00060_00000)

# find the time at which that discharge occurred at each site by filtering

# SP

SP_storm_time <- short_interval_data %>% 
  filter(site_no == 11113500,
         X_00060_00000 == 0.7)

#SP_storm_time
  
# Ven

Ven_storm_time <- short_interval_data %>% 
  filter(site_no == 11118500,
         X_00060_00000 == 1.08)

#Ven_storm_time
```

At Ventura, the stream flow peaked on 2021-10-05 at 7:25, the maximum flow was 1.08 ft^3/s. Ventura peaked for a longer time than Santa Paula. At Santa Paula, the stream flow peaked starting at 10:30 on 2021-10-05 and its peak value was 0.7 ft^3/s.

# Assignment 2.2: Metajam

Metajam helps you access the data and the metadata at the same time, it supports multiple repositories. When you cite the data, cite the package rather than just the .csv or other data document, this way you include all metadata in your citation.

```{r}
library(metajam)
library(here)
```

```{r, eval = FALSE}
# this method downloads the folder with files successfully here
# Import the data into its own chunk and set eval = FALSE so it doesnt run every time you knit this document.

data_url <- "https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8"

data_path <- download_d1_data(data_url, here())
# telling R to download the folder of data and metadata from a URL and then put the folder here()

# when you call the data, you only need to reference the folder with all the files within it, R knows which files are metadata and which are the dataframes

```

```{r}
#name the dataframe "data"
data <- read_d1_files("doi_10.5063_F1CJ8BPH__household_language__csv/")

# name the dataframe hh_data, and use $data to tell R to put it into dataframe form that we can read

hh_data <- data$data
```

## Create a plot by year from 2009 to 2015 showing the average percentage of household speaking only English at the State level (one data point per year)

```{r}
english_data <- hh_data %>% 
  filter(Year == 2009:2015) %>% 
  mutate(percentage_eng = (speak_only_english)/(total) *100) %>% 
  group_by(Year) %>% 
  summarize(mean  = mean(percentage_eng, na.rm = TRUE))

# check that the new col was created
#colnames(english_data)

#view the col
#percentage_eng_col <- english_data %>% 
 # select(percentage_eng)

english_plot <- ggplot(english_data, aes(x = Year, y = mean)) +
  geom_line(color = "coral1",
            size = 2) +
  scale_x_continuous(breaks = c(2009:2015)) +
 # scale_y_continuous(breaks = seq(60, 78, by = 2)) +
  scale_y_continuous(breaks = floor(english_data$mean)) +
  theme(panel.grid.major.y = element_line(size = 0.3, color = "black")) +
  labs(title = "Percentage of Alaskan Households That Only Speak English by Year",
       x = "Year",
       y = "Avg Percentage of Household Speaking Only English")

english_plot
```

According to this plot, the percentage of Alaskan households the speak only English peaked in 2010. After this peak, it declined sharply and fluctuated over the following years.

### Citations:

USGS National Water Dashboard: https://dashboard.waterdata.usgs.gov/app/nwd/?region=lower48&aoi=default

dataRetrieval package: https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html#daily-data

KNB repository for Alaska data: https://knb.ecoinformatics.org/view/doi:10.5063/F1N58JPP

Metajam package: https://cran.r-project.org/web/packages/metajam/index.html
















